{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170ea1b-7fa9-48df-b1d5-27a917a53309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook, gridplot\n",
    "from scipy.stats import linregress\n",
    "from bokeh.models import ColumnDataSource, Whisker, FactorRange\n",
    "from bokeh.layouts import grid, row, column\n",
    "from bokeh.transform import factor_cmap, jitter\n",
    "import seaborn as sns\n",
    "from bokeh.application import Application\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce532af9-55a2-4718-94c7-fd0f93180c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36a6f8d5-5648-414f-8d83-dc4d3f0b1764",
   "metadata": {},
   "source": [
    "# Overview \n",
    "\n",
    "\n",
    "\n",
    "We compare distributions of unit area runoff between pairs of measured locations to see if the \"closeness\" of the distributions can be predicted from a large set of attributes that describe the terrain, soil, land cover, and climate of each basin.  The analysis involves two main components/models:  \n",
    "* the quantization used to transform time series streamflow observations into discrete probability distributions, and\n",
    "* the Neural Network (NN) model used to predict the measure of difference between pairs of distributions from basin attributes.\n",
    "\n",
    "##  Limitations\n",
    "* Index representation of attributes,\n",
    "* Skewed distribution of basin attributes (drainage area is exponentially distributed),\n",
    "* Resolution of geospatial data (climate, $1 km$) compared to basin scale (min $1 km^2$),\n",
    "* Measurement uncertainty in runoff data being greater than reported precision.\n",
    "* Sample of observed locations is unevenly distributed in space, with a strong bias to the south of the province, meaning certain regions with unique hydrometeorological conditions are under-represented.\n",
    "* Minimum concurrent period of record for basin pair selection criteria leaves out valuable information by excluding comparisons and unique stations.\n",
    "* Maxiximum basin centroid distance adds selection bias, but excludes excess comparisons that might otherwise dilute the sample from more reasonable comparisons. I.e. allowing comparison of all pairs vs. comparison of mostly reasonable pairings that would easily be excluded, or artificially diluting the maximum rang.  On the other hand, the data should speak for itself?\n",
    "* attributes are probably not completely orthogonal (correlation between attributes)\n",
    "\n",
    "1) test how often the lowest DKL corresponds to the closest pair. (each basin gets compared with many other basins).\n",
    "2) add attributes one at a time and see how often the lowest DKL corresponds to the closest pair in (increasing dimension space).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf59e3-7da4-4e4c-be1c-81fd6563fb7a",
   "metadata": {},
   "source": [
    "## Looking to Graph Theory\n",
    "\n",
    ">Let a graph G of size N represent a network of two classes of nodes, 1 (blue) and 2 (green), where green is sparse in blue.  Let the edges (node connectivity) of a network graph G represent the shortest distance in B-dimensional space between each blue-green pair, where B is a set of unique attributes used to describe each node.   Let M represent a B by N matrix (adjacency) describing the connectivity of graph G.  What can graph theory offer to help interpret and understand how the graph connectivity changes as we vary the subset of attributes B used to compute the shortest distance?\n",
    "\n",
    "1. **Changes in Graph Topology**: By varying B, you may observe changes in the graph's topology, such as the formation or dissolution of edges, changes in the number of connected components, or the emergence of new clusters. Graph theory provides tools to quantify these changes, such as through the calculation of the number of connected components, the clustering coefficient, or measures of graph density.\n",
    "\n",
    "2. **Shortest Path Analysis**: The subset B* of attributes B directly affects the computation of the shortest paths between nodes (blue-green pairs in your case). Algorithms like Dijkstra's or the Floyd-Warshall algorithm can be used to find these shortest paths. Changes in B could lead to variations in the shortest path lengths and the actual paths taken, which can be analyzed to understand how different attributes influence connectivity.\n",
    "\n",
    "3. **Centrality Measures**: Graph theory offers various centrality measures (such as degree centrality, betweenness centrality, closeness centrality, and eigenvector centrality) to identify the most important nodes within the graph. Altering B could shift the centrality scores of nodes, indicating how changes in attribute consideration impact the relative importance or influence of nodes within the network.\n",
    "\n",
    "4. **Community Detection**: By changing B, the community structure within the graph may also change. Graph theory provides methods for community detection and modularity optimization, such as the Louvain method or Girvan-Newman algorithm, which can help identify how groups of nodes (communities) are formed or dissolved based on the attributes considered.\n",
    "\n",
    "5. **Robustness and Resilience**: The resilience of the network to failures or attacks can be assessed through graph theoretical measures. By varying B, you can study how the network's vulnerability to node or edge removal changes, providing insights into which attributes contribute to a more robust or fragile network structure.\n",
    "\n",
    "6. **Spectral Analysis**: The eigenvalues and eigenvectors of the adjacency matrix or the Laplacian matrix of the graph can provide insights into the graph's properties, such as connectivity, the existence of bipartite structures, or the presence of community structures. The spectral properties can change with variations in B, offering a mathematical lens through which to view the impact of attributes on overall network structure.\n",
    "\n",
    "7. **Visualization**: Graph theory supports the visualization of complex networks, allowing for the visual assessment of how changes in B affect the graph. Visual analysis can reveal patterns, clusters, outliers, or structural changes that are not immediately obvious through numerical analysis alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797e959-5e57-46cd-b8eb-a6a84875dc4a",
   "metadata": {},
   "source": [
    "## Visualization Concept\n",
    "\n",
    "* map of Vancouver Island.\n",
    "* blue lines indicate rivers,\n",
    "* triangles represent streamflow monitoring network,\n",
    "* blue dots indicate decision space (ungauged locations)\n",
    "  \n",
    "* use a small subset of observation network (triangles)\n",
    "* use unique colors for each triangle (station) to enhance visual contrast\n",
    "* the above is the \"basis arrangement\"\n",
    "\n",
    "* for each blue dot, find the \"closest\" station in 1d space  \n",
    "    * distance is spatial (xy) distance between basin polygon centroids\n",
    "* transform each blue dot into its corresponding proxy station shape/color\n",
    "* return to the \"basis arrangement\"\n",
    "* for each blue dot, repeat finding the \"nearest\" station in 2 to N dimensional space\n",
    " \n",
    "\n",
    "```\n",
    "Define ungauged basin attributes as A (N dimensional vector for each basin)\n",
    "Define decision space as M\n",
    "Define the set of streamflow monitoring locations as S.\n",
    "\n",
    "For D in range(1...N):\n",
    "    Define A_sub = A[1:D]\n",
    "    For each blue dot c in (M not S):\n",
    "        For each triangle t in S:\n",
    "            connect a line from blue dot c to triangle t (compute pairwise distance to each station)\n",
    "            emphasize the triangle representing the shortest distance\n",
    "\n",
    "    Convert all blue dots into their proxy shapes/colours\n",
    "\n",
    "    If D is not 1:\n",
    "        Note any changes in chosen proxies\n",
    "        store a visual snapshot of the proxy (store proxy rankings)\n",
    "\n",
    "```\n",
    "\n",
    "The above describes a network graph.  One issue is the ordering of A.  Is there an existing approach to evaluating how the graph connectivity changes as a function of the subset of A used to compute distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7dda70-ff9f-4185-9e17-efad82c1d515",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Sensitivity Analysis\n",
    "\n",
    "**Define a model arrangement to serve as a basis of comparison for sensitivity analysis.**\n",
    "\n",
    "Test the sensitivity to quantization by: \n",
    "1) varying dictionary size (4, 5, 6, 7, 8 bits),\n",
    "2) varying quantization approach (uniform & equiprobable),\n",
    "3) keep track of $D_{KL}$ values by pairs across 1 and 2 and look for trends.\n",
    "\n",
    "Test the NN model sensitivity to sample size:\n",
    "1)  There are 1507 observed locations, and 2.4M pairs (bi-directional comparisons since $D_{KL}$ is not symmetrical).\n",
    "2)  Of these pairs, ~150K have at least 10 years concurrent record and are not further than 500km apart (measured by basin polygon centroid distance).\n",
    "3)  Train NN with 1500, 1000, and 500 stations, hold # pairs constant use MC simulation to express the range of outcomes.\n",
    "\n",
    "Test the NN model sensitivity to sample bias in the geographic distribution of stations:\n",
    "1) Training the NN with geographic bias\n",
    "   * (leave out N from training set above 70 degrees North and hold\n",
    "  \n",
    "Test the sensitivity of NN structure:\n",
    "1) Define a basis model (look to other studies)\n",
    "2) Decrease \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3444aba-4a8c-40f3-a47d-881d716fb8f7",
   "metadata": {},
   "source": [
    "## The Kullback-Leibler Divergence\n",
    "\n",
    "The Kullback-Leibler divergence $D_{KL}$ is a measure of difference between two distributions.  The value of the measure represents the number of additional bits of memory needed to fully describe the observed data P (posterior/observed) given a model (prior/simulation) Q:\n",
    "\n",
    "$$D_{KL}(P ‖ Q) = \\sum_{i=0}^N P \\cdot log \\left(\\frac{P}{Q} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba470c-e442-433b-9aff-9daa6d0b9f30",
   "metadata": {},
   "source": [
    "\n",
    "One issue with the measure occurs where distributions don't overlap, which is often in the case of streamflow.  In many cases, sum(P) < sum(Q) after dropping i where p | q = 0, and we are often left with Pi<Qi yielding DKL < 0.  Or we may be misled into thinking DKL is small if we don't actually check the distributions.\n",
    "\n",
    "To get around this, my first instinct was to simply add 1 to the vector of bin counts (C* = C + [1,..., 1]), however the resulting C adds more information to the outliers.  It seems more reasonable to do C* = N*C + [1, ...,1]), where N is some multiple, I started with N=2.  This will change the entropy of both distributions, so I suppose we should track this change such that it can be compared at subsequent steps?  i.e. to ensure any effects are substantially greater than the information we've added.  We could also increase N, but I guess this represents a tradeoff in penalizing a certain characteristic of divergence vs. adding noise to the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb583544-6937-4756-ba37-4fe84b6a2958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "053d8281-dbb0-4f2d-bbaf-c7a869af2ced",
   "metadata": {},
   "source": [
    "## The Jensen-Shannon Divergence \n",
    "\n",
    "$$D_{JS}(P ‖ Q) = \\frac{1}{2} D_{KL}(P ‖ M) + \\frac{1}{2} D_{KL}(Q ‖ M)$$\n",
    "\n",
    "$$D_{JS-\\text{LOW FLOW}}(P_1 ‖ Q_1) = \\frac{1}{2} D_{KL}(P_1 ‖ M_1) + \\frac{1}{2} D_{KL}(Q_1 ‖ M_1)$$\n",
    "\n",
    "$$D_{JS-\\text{HIGH FLOW}}(P_N ‖ Q_N) = \\frac{1}{2} D_{KL}(P_N ‖ M_N) + \\frac{1}{2} D_{KL}(Q_N ‖ M_N) \\text{ where } N =2^{\\text{BITRATE}}$$\r\n",
    "\r\n",
    "where D is the Kullback-Leibler divergence, and M is the average of P and Q, defineas:d \n",
    "\n",
    "asQ)(M = \\frac{1}{2}(P + Q)$$\n",
    "\n",
    "The Jensen-Shannon Divergence can be expressed as:\n",
    "\n",
    "$$JSD(P ‖ Q) = \\frac{1}{2} \\sum_{i=0}^N P \\cdot log \\left( \\frac{P}{M} \\right) + \\frac{1}{2} \\sum_{i=0}^N Q \\cdot log \\left( \\frac{Q}{M} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57ba2d-bc34-4917-8f0e-6827763115b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3860c8b-0ff1-44af-a5f2-deac172eaec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_jdf_model_result(model, df, b):\n",
    "    last_col = 2**b - 1\n",
    "    jdf = df[f'jsd_{model}'].str.split(', ', expand=True)\n",
    "    # print(jdf[[0, last_col]].head())\n",
    "    jdf[0] = jdf[0].apply(lambda r: r.split('[')[1])\n",
    "    jdf[last_col] = jdf[last_col].apply(lambda r: r.split(']')[0])\n",
    "    \n",
    "    for c in jdf.columns:\n",
    "        jdf[c] = jdf[c].astype(float)\n",
    "    jdf.columns = [f'{c:03d}' for c in jdf.columns]\n",
    "    \n",
    "    low_flow_cols = list(jdf.columns)[:2]\n",
    "    high_flow_cols = list(jdf.columns)[-2:]\n",
    "    mid_range_cols = [e for e in list(jdf.columns) if e not in low_flow_cols + high_flow_cols]\n",
    "    for quantile_focus in [f'low_flow_jsd_{model}', f'mid_range_jsd_{model}', f'high_flow_jsd_{model}']:\n",
    "        if quantile_focus.startswith('low'):\n",
    "            df[quantile_focus] = jdf.loc[:, low_flow_cols].sum(1)\n",
    "        elif quantile_focus.startswith('mid'):\n",
    "            df[quantile_focus] = jdf.loc[:, mid_range_cols].sum(1)\n",
    "        else:\n",
    "            df[quantile_focus] = jdf.loc[:, high_flow_cols].sum(1)\n",
    "    return df, jdf\n",
    "\n",
    "def reformulate_dkl_model_result(model, metric, df, b):\n",
    "    last_col = 2**b - 1\n",
    "    col = f'{model}_{metric}'\n",
    "        \n",
    "    ddf = df[f'{col}_disagg'].str.split(', ', expand=True)\n",
    "    ddf[0] = ddf[0].apply(lambda r: r.split('[')[1])\n",
    "    ddf[last_col] = ddf[last_col].apply(lambda r: r.split(']')[0])\n",
    "    \n",
    "    for c in ddf.columns:\n",
    "        ddf[c] = ddf[c].astype(float)\n",
    "    ddf.columns = [f'{c:03d}' for c in ddf.columns]\n",
    "    \n",
    "    low_flow_cols = list(ddf.columns)[:2]\n",
    "    high_flow_cols = list(ddf.columns)[-2:]\n",
    "    mid_range_cols = [e for e in list(ddf.columns) if e not in low_flow_cols + high_flow_cols]\n",
    "    for quantile_focus in [f'low_flow_{metric}_{model}', f'mid_range_{metric}_{model}', f'high_flow_{metric}_{model}']:\n",
    "        if quantile_focus.startswith('low'):\n",
    "            df[quantile_focus] = ddf.loc[:, low_flow_cols].sum(1)\n",
    "        elif quantile_focus.startswith('mid'):\n",
    "            df[quantile_focus] = ddf.loc[:, mid_range_cols].sum(1)\n",
    "        else:\n",
    "            df[quantile_focus] = ddf.loc[:, high_flow_cols].sum(1)\n",
    "    return df, ddf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a695f3b-bd63-4c80-8bae-0989c2474a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.weight_history = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Assuming model has a single layer or specifying the layer of interest\n",
    "        weights = self.model.layers[0].get_weights()[0] # Adjust layer index as needed\n",
    "        self.weight_history.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec6502-e05c-4360-a3f6-14e440246f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(dataframe, feature_columns, target_column):\n",
    "    # Split the data into features and target\n",
    "    X = dataframe[feature_columns].values\n",
    "    y = dataframe[target_column].values\n",
    "    print(f'{len(feature_columns)} feature columns')\n",
    "\n",
    "    # Standardize the features\n",
    "    # scaler = StandardScaler()\n",
    "    scaler = QuantileTransformer(output_distribution='normal')\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "       # Define the model\n",
    "    input_size = X_train.shape[1]\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(2, activation='relu', input_shape=(input_size,), kernel_initializer='he_uniform'), # input layer with (input size) features\n",
    "        layers.Dropout(0.5), # dropout for regularization\n",
    "        # layers.Dense(2, activation='relu'), # hidden layer\n",
    "        layers.Dropout(0.5), # dropout for regularization\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    # model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mae'])\n",
    "\n",
    "    return model, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14fa75-e0e1-4824-95dd-2052d30fcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_DATA_DIR = '/home/danbot2/code_5820/large_sample_hydrology/common_data/HYSETS_data/'\n",
    "hs_properties_path = os.path.join(HS_DATA_DIR, 'HYSETS_watershed_properties.txt')\n",
    "hs_df = pd.read_csv(hs_properties_path, sep=';')\n",
    "hs_df.set_index('Official_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a67ba1-3094-439f-949f-258693002b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coords(stn, col):\n",
    "    return hs_df.loc[stn, col]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51981d40-4f8d-4d78-9a63-cbe5677ff517",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 6\n",
    "# bin_model = 'equiprobable'\n",
    "metric = 'tvd'\n",
    "metric = 'dkl'\n",
    "bin_model = 'uniform'\n",
    "# metric = 'tvd'\n",
    "target_column = f'{bin_model}_{metric}'\n",
    "fname = f'compression_test_results_{b}bits_20240212.csv'\n",
    "fname = f'DKL_test_results_{b}bits_20240212.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d06c1-2ce6-4e57-a4bb-b1de86cca330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('compression_test_results', fname))\n",
    "print(f'{len(df)} samples in the results file')\n",
    "# print(sorted(df.columns))\n",
    "mcols = [c for c in df.columns if metric in c]\n",
    "df[mcols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c7b15-683d-47fe-9032-b5038a07025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scale the drainage areas\n",
    "da_cols = [c for c in df.columns if 'Drainage_Area' in c]\n",
    "for c in da_cols:\n",
    "    df[c] = np.log10(df[c].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8da303-23e7-4524-8e93-84ca4b88dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[[c for c in df.columns if 'jsd' in c]].head()\n",
    "# df.columns\n",
    "minc, maxc = min(df[target_column]), max(df[target_column])\n",
    "print(f'{target_column} min: {minc:.2f} max: {maxc:.2f}')\n",
    "neg_dkl = df[df[target_column] < 0].copy()\n",
    "neg_dkl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbee6c-8497-4413-b949-a9a4818ce77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in ['Centroid_Lat_deg_N', 'Centroid_Lon_deg_E']:\n",
    "#     df[f'proxy_{c}'] = df['proxy'].apply(lambda s: add_coords(s, c))\n",
    "#     df[f'target_{c}'] = df['target'].apply(lambda s: add_coords(s, c))\n",
    "\n",
    "# df.to_csv(os.path.join('compression_test_results', fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43d200-a7eb-42a2-b6d4-c853ceff8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, jdf = reformulate_model_result(model, df, b)\n",
    "if metric != 'tvd':\n",
    "    df, jdf = reformulate_dkl_model_result(bin_model, metric, df, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa8833-cc3c-4657-9690-4af0fe0e4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'Centroid_Lat_deg_N', 'Centroid_Lon_deg_E',\n",
    "    'Drainage_Area_km2',\n",
    "    'Elevation_m', 'Slope_deg',\n",
    "    'Aspect_deg',\n",
    "    'Gravelius', 'Perimeter',\n",
    "    'Land_Use_Forest_frac', 'Land_Use_Grass_frac', 'Land_Use_Wetland_frac',\n",
    "    'Land_Use_Snow_Ice_frac', 'Land_Use_Urban_frac', 'Land_Use_Shrubs_frac',\n",
    "    'Land_Use_Crops_frac', 'Land_Use_Water_frac',\n",
    "    'Permeability_logk_m2', 'Porosity_frac',\n",
    "    'tmax', 'tmin',\n",
    "    'prcp',\n",
    "    'srad', 'swe', 'vp',\n",
    "    'high_prcp_freq',\n",
    "    'high_prcp_duration',\n",
    "    'low_prcp_freq',\n",
    "    'low_prcp_duration', \n",
    "]\n",
    "\n",
    "feature_cols = [f'proxy_{c}' for c in feature_columns]\n",
    "feature_cols += [f'target_{c}' for c in feature_columns]\n",
    "\n",
    "# if target_column.startswith('low_flow'):\n",
    "#     feature_columns = [e for e in feature_columns if not e.startswith('high_prcp')]\n",
    "# elif target_column.startswith('high_flow'):\n",
    "#     feature_columns = [e for e in feature_columns if not e.startswith('low_prcp')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3c3b8-45d4-4986-bcbf-415097ec6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = df[feature_cols + ['centroid_distance'] + [target_column]].copy().dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bde3e-5509-4c9e-ba3a-eed9780b33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, X_train, y_train, X_test, y_test = prepare_model(input_df, feature_cols + ['centroid_distance'], target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271bb915-936b-495c-b1ca-9ec7e0a07dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Create a log directory with a unique name\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n",
    "weight_history_callback = WeightHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35707a-1186-4f43-895b-b5af4d172904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "epochs = 20 # Example number of epochs\n",
    "cb_list = [tensorboard_callback, weight_history_callback, early_stopping_callback]\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks=cb_list, \n",
    "          validation_split=0.2)\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219af9d6-5158-4e18-9c8b-f60e479a0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22793c9e-628b-405f-be38-aefb4f74a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wh = weight_history_callback.weight_history\n",
    "# Assuming `weight_history` is collected appropriately\n",
    "epochs = range(len(wh))\n",
    "features = range(len(wh[0]))\n",
    "\n",
    "# Convert weight history to a 2D array for heatmap generation\n",
    "weight_matrix = np.array([w.flatten() for w in wh])\n",
    "plt.figure(figsize=(16, 5))\n",
    "sns.heatmap(weight_matrix, yticklabels=epochs, cmap=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Epoch')\n",
    "plt.title(f'Heatmap of Weight Changes ({len(wh[0])} features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd2576-0d6a-4774-9441-7abaa6e90c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(wh)):\n",
    "#     data = wh[i].flatten()\n",
    "#     print(len(data))\n",
    "#     indices_of_largest = np.argsort(np.abs(data))[-5:].flatten()\n",
    "#     largest_weights = [data[n] for n in indices_of_largest]\n",
    "#     feats = [feature_cols[n] for n in indices_of_largest]\n",
    "#     notes = [f'{\"_\".join(f.split(\"_\")[1:])}: {w:.2f}' for w, f in zip(largest_weights, feats)]\n",
    "#     print(f'Epoch {i}: ', ', '.join(notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14792b52-d165-42ca-ac07-3667ac102dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687612a-937d-4bde-b2fa-69bde0b4d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'model' is your trained model and X_test, y_test are your test data\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0ce0c-4b3f-47b8-9fd9-d1a3c1b43b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vertical_histogram(residuals, bitrate, r, NBINS='auto'):\n",
    "        \n",
    "    # create the vertical histogram        \n",
    "    vhist, vedges = np.histogram(residuals['residuals'], bins=NBINS, density=True)\n",
    "    vzeros = np.zeros(len(vedges)-1)\n",
    "    vmax = max(vhist)*1.1\n",
    "    \n",
    "    LINE_ARGS = dict(color=\"#3A5785\", line_color=None)\n",
    "    pv = figure(toolbar_location='above', width=200, height=350, x_range=(0, vmax),\n",
    "                y_axis_location='right', y_range=r.y_range)\n",
    "    pv.ygrid.grid_line_color = None\n",
    "    pv.xaxis.major_label_orientation = np.pi/4\n",
    "    pv.background_fill_color = \"#fafafa\"\n",
    "    \n",
    "    pv.quad(left=0, bottom=vedges[:-1], top=vedges[1:], right=vhist, color=\"white\", line_color=\"#3A5785\")\n",
    "    vh1 = pv.quad(left=0, bottom=vedges[:-1], top=vedges[1:], right=vzeros, alpha=0.5, **LINE_ARGS)\n",
    "    # vh2 = pv.quad(left=0, bottom=vedges[:-1], top=vedges[1:], right=vzeros, alpha=0.1, **LINE_ARGS)\n",
    "    pv.line([0, max(vhist)], [0, 0], color='red', line_dash='dashed', line_width=3)\n",
    "    # plot the 25 & 75 percentile lines\n",
    "    pct1 = np.percentile(residuals['residuals'], 2.5)\n",
    "    pct2 = np.percentile(residuals['residuals'], 98.5)\n",
    "    pv.line([0, 0.1*max(vhist)], [pct1, pct1], color='orange', line_dash='dashed', line_width=3)\n",
    "    pv.line([0, 0.1*max(vhist)], [pct2, pct2], color='orange', line_dash='dashed', line_width=3)\n",
    "    pmed = np.percentile(residuals['residuals'], 50)\n",
    "    pv.line([0, np.max(vhist)], [pmed, pmed], color='blue', line_dash='dashed', line_width=3)\n",
    "    return pv\n",
    "\n",
    "def create_horizontal_histogram(residuals, bitrate, r, NBINS='auto'):\n",
    "        \n",
    "    # create the vertical histogram        \n",
    "    hist, edges = np.histogram(residuals['predicted'], bins=NBINS, density=True)\n",
    "    zeros = np.zeros(len(edges)-1)\n",
    "    hmax = max(hist)*1.1\n",
    "    \n",
    "    LINE_ARGS = dict(color=\"#3A5785\", line_color=None)\n",
    "    ph = figure(toolbar_location=None, width=400, height=200, y_range=(0, hmax),\n",
    "                x_range=r.x_range)\n",
    "    ph.ygrid.grid_line_color = None\n",
    "    ph.xaxis.major_label_orientation = np.pi/4\n",
    "    ph.background_fill_color = \"#fafafa\"\n",
    "    \n",
    "    ph.quad(left=edges[:-1], bottom=0, top=hist, right=edges[1:], color=\"white\", line_color=\"#3A5785\")\n",
    "    h1 = ph.quad(left=edges[:-1], bottom=0, top=zeros, right=edges[1:], alpha=0.5, **LINE_ARGS)\n",
    "    # vh2 = pv.quad(left=0, bottom=vedges[:-1], top=vedges[1:], right=vzeros, alpha=0.1, **LINE_ARGS)\n",
    "    pmed = np.percentile(residuals['predicted'], 50)\n",
    "    ph.line([pmed, pmed], [0, max(hist)], color='red', line_dash='dashed', line_width=3)\n",
    "    # plot the 25 & 75 percentile lines\n",
    "    # pct1 = np.percentile(residuals['predicted'], 2.5)\n",
    "    # pct2 = np.percentile(residuals['predicted'], 98.5)\n",
    "    # ph.line([pct1, pct1], [0, 0.1*max(hist)], color='orange', line_dash='dashed', line_width=3)\n",
    "    # ph.line([0, 0.1*max(hist)], [pct2, pct2], color='orange', line_dash='dashed', line_width=3)\n",
    "    # \n",
    "    # ph.line([0, np.max(hist)], [pmed, pmed], color='blue', line_dash='dashed', line_width=3)\n",
    "    return ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb43c6-c068-47a2-9f8b-d13a8355ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_best_fit(X, Y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Compute the best fit line based on L1 norm (Least Absolute Deviations).\n",
    "    \n",
    "    :param X: Independent variable (e.g., actual values)\n",
    "    :param Y: Dependent variable (e.g., predicted values)\n",
    "    :return: Slope and intercept of the best fit line\n",
    "    \"\"\"\n",
    "    # Objective function to minimize (sum of absolute differences)\n",
    "    def objective_line(params):\n",
    "        a, b = params\n",
    "        return np.sum(np.abs(Y - (a * X + b)))\n",
    "\n",
    "    # Initial guess for slope (a) and intercept (b)\n",
    "    initial_guess = [0, np.median(Y)]\n",
    "\n",
    "    # Minimize the objective function\n",
    "    result = minimize(objective_line, initial_guess, method='Powell')\n",
    "\n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(X, Y)\n",
    "\n",
    "    return result.x, r_value**2, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdb88a-b090-43c2-adcf-e1a16d959854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_band(x, y):\n",
    "    if len(y) == 0:\n",
    "        raise Exception('y is empty')\n",
    "    rdf = pd.DataFrame({'x': x, 'y':y})\n",
    "    results = []\n",
    "    sorted_x = sorted(x)\n",
    "    n_groups = 10\n",
    "    for grp in np.array_split(sorted_x, n_groups):        \n",
    "        grp_min, grp_max = min(grp), max(grp)\n",
    "        data = rdf[(rdf['x'] >= grp_min) & (rdf['x'] < grp_max)].copy()\n",
    "        y_vals = data['y'].values\n",
    "        if len(y_vals) < 25:\n",
    "            print(f'bin too small! ({grp_min}-{grp_max}) (N={len(y_vals)})')\n",
    "        x_position = np.median(grp)\n",
    "        median = np.median(y_vals)\n",
    "        ci_lower = np.percentile(y_vals, 2.5)\n",
    "        ci_upper = np.percentile(y_vals, 97.5)\n",
    "        results.append((x_position, (grp_min + grp_max)/2, ci_lower, median, ci_upper))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638de513-c377-4a1e-84a4-114172007aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_variance_by_prediction(Y, X, n_bootstrap=1000, n_bins=20):\n",
    "    \"\"\"\n",
    "    Estimates the variance of residuals as a function of predicted values\n",
    "    using bootstrap resampling.\n",
    "\n",
    "    Parameters:\n",
    "    - p: array-like, predicted values.\n",
    "    - s: array-like, observed (simulated) values.\n",
    "    - n_bootstrap: int, number of bootstrap samples to generate.\n",
    "    - n_bins: int, number of bins to divide the predicted values into.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with the bin centers, mean variance, and 95% confidence intervals for each bin.\n",
    "    \"\"\"\n",
    "    # Calculate initial residuals\n",
    "    residuals = X - Y\n",
    "\n",
    "    equiprobable_bins = np.array_split(sorted(X), n_bins)\n",
    "    bin_edges = [min(X)]\n",
    "    b = 1\n",
    "    bin_labels = []\n",
    "    for grp in equiprobable_bins:\n",
    "        bin_edges.append(max(grp))\n",
    "        bin_labels.append(f'{bin_edges[b-1]:.2f}-{bin_edges[b]:.2f}')\n",
    "        b += 1\n",
    "\n",
    "    # Bin the predicted values\n",
    "    bin_centers = np.add(bin_edges[:-1], bin_edges[1:]) / 2\n",
    "\n",
    "    # DataFrame to store results\n",
    "    results = pd.DataFrame({\n",
    "        'bin_centre': bin_centers,\n",
    "        'median': np.zeros(n_bins),\n",
    "        'ci_lower': np.zeros(n_bins),\n",
    "        'ci_upper': np.zeros(n_bins),\n",
    "    })\n",
    "\n",
    "    # Bootstrap resampling\n",
    "    for i in range(n_bins):\n",
    "        observed = []\n",
    "        # Select observed values that fall into the current bin\n",
    "        mask = (Y >= bin_edges[i]) & (Y < bin_edges[i+1])\n",
    "        bin_vals = X[mask]\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            # Resample observed values within the bin with replacement\n",
    "            resample = np.random.choice(bin_vals, size=len(bin_vals), replace=True)\n",
    "            observed.extend(resample)\n",
    "\n",
    "        if observed:\n",
    "            # Calculate 95% confidence interval from the bootstrap samples\n",
    "            results.at[i, 'ci_lower'], results.at[i, 'ci_upper'] = np.percentile(observed, [2.5, 97.5])\n",
    "            results.at[i, 'median'], results.at[i, 'bin_centre'] = np.percentile(observed, 50), bin_centers[i]\n",
    "        else:\n",
    "            # Handle bins with no data by setting NaN values\n",
    "            results.at[i, 'ci_lower'] = np.nan\n",
    "            results.at[i, 'ci_upper'] = np.nan\n",
    "            \n",
    "    results['Category'] = bin_labels\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8316f6-2ed8-45c2-8f3f-552957c20991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a hook function to rotate x-axis labels\n",
    "def rotate_x_labels(plot, element):\n",
    "    plot.state.xaxis.major_label_orientation = np.pi / 4  # 45 degrees in radians\n",
    "\n",
    "def create_violin_plot(X, Y, n_bins=20):\n",
    "    df = pd.DataFrame({'Predicted': X, 'Observed': Y})\n",
    "    \n",
    "    equiprobable_bins = np.array_split(sorted(X), n_bins)\n",
    "    bin_edges = [min(X)]\n",
    "    b = 1\n",
    "    bin_labels = []\n",
    "    for grp in equiprobable_bins:\n",
    "        bin_edges.append(max(grp))\n",
    "        bin_labels.append(f'{bin_edges[b-1]:.2f}-{bin_edges[b]:.2f}')\n",
    "        b += 1\n",
    "    \n",
    "    df['Predicted Category'] = pd.cut(df['Predicted'], bins=bin_edges, labels=bin_labels)\n",
    "    \n",
    "    # Create a violin plot\n",
    "    violin_plot = hv.Violin(df, ('Predicted Category', 'Predicted Bins'), 'Observed').sort()\n",
    "    violin_plot = violin_plot.relabel(group='Quartiles').opts(opts.Violin(inner='quartiles', cut=1., bandwidth=1))\n",
    "    violin_plot.opts(\n",
    "        tools=['hover'], width=1000, height=400, \n",
    "        xlabel='Predicted Value Bins', ylabel='Observed Values',\n",
    "        hooks=[rotate_x_labels])\n",
    "    return violin_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b4483-c96a-4131-8397-ea29303f00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_with_trendline(predicted, actual, bitrate, model):\n",
    "    # Activate inline plotting in the notebook\n",
    "\n",
    "    x1, x2 = min(predicted), max(predicted)\n",
    "    y1, y2 = min(actual), max(actual)\n",
    "\n",
    "    # get the L1 best fit parameters\n",
    "    (m, b), r2, pval = l1_best_fit(predicted, actual)\n",
    "\n",
    "    lx = np.linspace(x1, x2, 100)\n",
    "    ly = [m*e + b for e in lx]\n",
    "\n",
    "    # Create a new plot with a title and axis labels\n",
    "    title = f\"{bitrate} bit Predicted vs Observed {model} (N={len(actual)})\"\n",
    "    p = figure(title=title, width=400, height=350,\n",
    "               y_axis_label=f'Observed {metric}', \n",
    "               x_axis_label=f'Predicted {metric}',\n",
    "              # x_axis_type='log', y_axis_type='log'\n",
    "              )\n",
    "    \n",
    "    # plot confidence interval\n",
    "    plot_ci = False\n",
    "    # plot_ci = True\n",
    "    if plot_ci:\n",
    "        # bin_medians, bin_midpoints, lb, median, ub = zip(*plot_confidence_band(predicted, actual))   \n",
    "        var_df = bootstrap_variance_by_prediction(predicted, actual)\n",
    "        vplot = create_violin_plot(predicted, actual)\n",
    "        # bin_medians, bin_midpoints = var_df['bin_centre'], \n",
    "        # print(var_df.head())\n",
    "        bin_medians = var_df['bin_centre'].values\n",
    "        lb, ub, median = var_df['ci_lower'].values, var_df['ci_upper'].values, var_df['median'].values\n",
    "        # print(asdfd)\n",
    "\n",
    "    # Add scatter plot\n",
    "    p.circle(predicted, actual, size=5, color=\"navy\", alpha=0.25, legend_label='pts')\n",
    "\n",
    "    vplot = None\n",
    "    if plot_ci:\n",
    "        # mx_adjusted = list(bin_midpoints)\n",
    "        mx_adjusted = list(bin_medians)\n",
    "        # mx_adjusted[0] -= 0.05\n",
    "        p.varea(mx_adjusted, y1=lb, y2=ub, color='green', alpha=0.3, legend_label='95% CI')\n",
    "\n",
    "    # plot the 1:1 line between actual vs. predicted\n",
    "    p.line([0, max(predicted)], [0, max(predicted)], color='red', line_dash='dashed', line_width=3,\n",
    "           legend_label='1:1')\n",
    "\n",
    "    p.line(lx, ly, color='skyblue', line_dash='dashed', line_width=3,\n",
    "           legend_label=f'R²={r2:.2f}')\n",
    "\n",
    "    if plot_ci:\n",
    "        p.circle(bin_medians, median, color='lawngreen', size=7, legend_label='median')\n",
    "    p.legend.location = 'top_left'\n",
    "    p.legend.background_fill_alpha = 0.7\n",
    "    p.legend.click_policy = 'hide'\n",
    "\n",
    "    title = f\"{bin_model} Model Residuals (N={len(actual)}, {bitrate} bits)\"\n",
    "    r = figure(title=title, width=400, height=350,\n",
    "               y_axis_label=f'Observed {bin_model}', \n",
    "               x_axis_label=f'Predicted {bin_model}',\n",
    "              toolbar_location='above')\n",
    "\n",
    "    residuals = pd.DataFrame()\n",
    "    residuals['predicted'] = predicted\n",
    "    residuals['observed'] = actual\n",
    "    residuals['residuals'] = predicted - actual\n",
    "    minr, maxr = residuals['observed'].min(), residuals['observed'].max()\n",
    "\n",
    "    r.circle(residuals['predicted'], residuals['residuals'], \n",
    "             color='orange', size=2, alpha=0.4, )\n",
    "    r.line([minr, maxr], [0, 0], color='red', line_width=3, line_dash='dashed')\n",
    "    r.yaxis.axis_label = f'Residuals (pred - obs {metric})'\n",
    "    r.xaxis.axis_label = f'Predicted {metric}'\n",
    "\n",
    "    hp = create_vertical_histogram(residuals, bitrate, r)\n",
    "    hh = create_horizontal_histogram(residuals, bitrate, r)\n",
    "    \n",
    "    return row(p, column(r, hh), hp), vplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127abd2-a11e-4672-b6d2-a6f30e100a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot, vplot = plot_predictions_with_trendline(predictions.flatten(), y_test, b, bin_model)\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c496115-9977-4f6d-8ef3-a6fea90df18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot, vplot = plot_predictions_with_trendline(predictions.flatten(), y_test, b, bin_model)\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32349ad9-ff9d-4ff0-86b4-aa39f46020a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot, vplot = plot_predictions_with_trendline(predictions.flatten(), y_test, b, bin_model)\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3beb0-e242-45d7-8cf8-402841e50514",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot, vplot = plot_predictions_with_trendline(predictions.flatten(), y_test, b, bin_model)\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e13bd4-0a3e-40a8-b479-9dc7b9e2a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(dataframe, num_cols=5):\n",
    "    \"\"\"\n",
    "    Creates a grid plot of histograms for each feature in the dataframe using Bokeh.\n",
    "    \n",
    "    :param dataframe: A pandas DataFrame containing the normalized features.\n",
    "    :param num_cols: Number of columns in the grid plot.\n",
    "    \"\"\"\n",
    "\n",
    "    num_features = dataframe.shape[1]\n",
    "    num_rows = np.ceil(num_features / num_cols).astype(int)\n",
    "\n",
    "    plots = []\n",
    "    for col in dataframe.columns:\n",
    "        hist, edges = np.histogram(dataframe[col], bins=20, density=True)\n",
    "        p = figure(title=f'Distribution of {col}', tools='', background_fill_color=\"#fafafa\")\n",
    "        p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n",
    "        plots.append(p)\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8fdbd-e600-4fe5-9f46-469bf0c9358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a03d7c-e96b-4e59-99b9-5c6f5baaf1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_loc = 'proxy'\n",
    "ccs = [f'{which_loc}_{c}' for c in feature_columns] + ['centroid_distance', target_column]\n",
    "# ccs = [c.split('_frac')[0] for c in ccs]\n",
    "print(ccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615de163-36b9-4ce6-9c6f-c8e43243c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[target_column] = np.sqrt(df[target_column])\n",
    "\n",
    "ddf = df[ccs].copy().dropna()\n",
    "\n",
    "ddf.head()\n",
    "ddf['centroid_distance'].max()\n",
    "# ddf['centroid_distance'] = np.sqrt(ddf['centroid_distance'])\n",
    "scaled_df = (ddf[ccs] - ddf[ccs].min()) / (ddf[ccs].max() - ddf[ccs].min())\n",
    "# ddf[['centroid_distance']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c89b1c-20e2-4c5b-b1f5-61e67861d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = plot_feature_distributions(scaled_df)\n",
    "# Create a grid layout of plots\n",
    "grid_layout = gridplot(plots, ncols=4, width=225, height=225)\n",
    "\n",
    "# Show the grid layout\n",
    "show(grid_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817003fa-e1ff-421e-9607-74b273798640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the breakdown of JSD as a function of flow\n",
    "b = 4\n",
    "fname = f'compression_test_results_{b}bits_20240212.csv'\n",
    "fname = f'DKL_test_results_{b}bits_20240212.csv'\n",
    "df = pd.read_csv(os.path.join('compression_test_results',fname))\n",
    "print(f'{len(df)} samples in the results file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebf11f-aec5-4d21-8627-97abd6d07640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'equiprobable'\n",
    "model = 'uniform'\n",
    "df, jdf = reformulate_dkl_model_result(model, 'dkl', df, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89c36e-463c-48df-b78f-d348bbc43cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = jdf.copy().reset_index().melt(id_vars='index')\n",
    "df_long.columns = ['index', 'group', 'value']\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfe30e-9c72-4805-8fed-e52750c86c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sorted(list(set(jdf.columns)))\n",
    "jdf.dropna(how='any', inplace=True)\n",
    "medians = [np.percentile(jdf[c].values, 50) for c in jdf.columns]\n",
    "lb = [np.percentile(jdf[c].values, 2.5) for c in jdf.columns]\n",
    "ub = [np.percentile(jdf[c].values, 98.5) for c in jdf.columns]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd608e8-ae11-4dd0-ae0b-158031270822",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(data=dict(base=x, upper=ub, lower=lb))\n",
    "error = Whisker(base=\"base\", upper=\"upper\", lower=\"lower\", source=source,\n",
    "                level=\"annotation\", line_width=0.5)\n",
    "x_range = FactorRange(factors=x)\n",
    "\n",
    "title=f\"{model[0].upper() + model[1:]} KL Divergence by Quantization Level\"\n",
    "\n",
    "q = figure(width=600, height=350, title=title,\n",
    "           output_backend='webgl', x_range=x_range)\n",
    "q.circle(jitter(\"group\", 0.3, range=x_range), \"value\", source=df_long,\n",
    "         alpha=0.5, size=2, line_color=\"white\",\n",
    "         color='dodgerblue')\n",
    "q.circle(x, medians, size=10)\n",
    "q.add_layout(error)\n",
    "\n",
    "\n",
    "show(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429a7e3-6ff2-422b-ba19-e294c6b09e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(data=dict(base=x, upper=ub, lower=lb, median=medians))\n",
    "error = Whisker(base=\"base\", upper=\"upper\", lower=\"lower\", source=source,\n",
    "                level=\"annotation\", line_width=0.5)\n",
    "x_range = FactorRange(factors=x)\n",
    "\n",
    "title=f\"{model[0].upper() + model[1:]} Jensen-Shannon Divergence by Quantization Level\"\n",
    "\n",
    "q = figure(width=600, height=350, title=title,\n",
    "           output_backend='webgl', x_range=x_range)\n",
    "q.circle('base', 'median', size=2, source=source)\n",
    "# q.circle(jitter(\"group\", 0.3, range=x_range), \"value\", source=df_long,\n",
    "#          alpha=0.5, size=2, line_color=\"white\",\n",
    "#          color='dodgerblue')\n",
    "\n",
    "q.add_layout(error)\n",
    "\n",
    "\n",
    "show(q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5084bab7-1b8e-4edb-b264-16c0ec89a934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0148d19-bdef-48f5-8a0e-1a6d9b8d8a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
